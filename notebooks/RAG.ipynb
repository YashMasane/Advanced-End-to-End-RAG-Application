{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3781c7e1",
   "metadata": {},
   "source": [
    "# **Building a simple RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69031c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.5'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66f71d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "# loading the environment variables\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41076705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "loader = TextLoader(\"../data/AI.txt\", encoding='utf-8')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7377ca07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/AI.txt'}, page_content='The Evolution and Impact of Artificial Intelligence: From Theory to Practice\\n\\nArtificial Intelligence (AI) has transformed from a theoretical concept in the 1950s into one of the most influential technologies of the 21st century. The field encompasses multiple disciplines including machine learning, natural language processing, computer vision, and robotics, with applications spanning healthcare, finance, education, and beyond.\\nHistorical Foundations and Key Milestones\\n\\nThe term \"Artificial Intelligence\" was coined by John McCarthy in 1956 at the Dartmouth Conference, marking the official birth of AI as an academic discipline. Early pioneers like Alan Turing proposed the famous Turing Test in 1950, asking whether machines could exhibit intelligent behavior indistinguishable from humans. The first AI programs emerged in the late 1950s, including the Logic Theorist (1956) and the General Problem Solver (1957).\\n\\nThe field experienced its first major setback during the \"AI Winter\" of 1974-1980, when funding dried up due to unmet expectations and computational limitations. A second AI Winter occurred from 1987-1993, triggered by the collapse of the Lisp machine market. However, these challenging periods led to more realistic assessments and eventually stronger foundations for future breakthroughs.\\nMachine Learning: The Engine of Modern AI\\n\\nMachine learning (ML) represents the dominant paradigm in contemporary AI, enabling systems to learn from data without explicit programming. The field divides into three primary categories: supervised learning, unsupervised learning, and reinforcement learning.\\n\\nSupervised learning algorithms train on labeled datasets, learning to map inputs to outputs. Popular techniques include linear regression, decision trees, random forests, and support vector machines (SVMs). Deep learning, a subset of supervised learning, uses artificial neural networks with multiple layers—sometimes reaching depths of 100+ layers in architectures like ResNet-152. The ImageNet competition in 2012 marked a watershed moment when AlexNet achieved a 15.3% error rate, dramatically outperforming previous approaches that hovered around 26%.\\n\\nUnsupervised learning discovers hidden patterns in unlabeled data through techniques like clustering (K-means, DBSCAN) and dimensionality reduction (PCA, t-SNE). These methods have proven invaluable for customer segmentation, anomaly detection, and exploratory data analysis across industries.\\n\\nReinforcement learning (RL) trains agents to make sequential decisions by rewarding desired behaviors. DeepMind\\'s AlphaGo demonstrated RL\\'s potential in 2016 by defeating world champion Lee Sedol 4-1 in the ancient game of Go, a feat previously thought to be decades away. More recently, OpenAI\\'s GPT models and DeepMind\\'s AlphaFold have showcased the power of combining RL with other techniques.\\nNatural Language Processing: Teaching Machines to Understand Human Language\\n\\nNatural Language Processing (NLP) enables computers to comprehend, interpret, and generate human language. Early NLP systems relied on rule-based approaches and hand-crafted features, requiring extensive linguistic expertise. The statistical revolution in the 1990s introduced probabilistic models and machine learning, improving performance significantly.\\n\\nThe transformer architecture, introduced in the seminal 2017 paper \"Attention Is All You Need\" by Vaswani et al., revolutionized NLP. Transformers use self-attention mechanisms to process entire sequences simultaneously, enabling parallel computation and capturing long-range dependencies more effectively than recurrent neural networks (RNNs). This architecture spawned groundbreaking models like BERT (Bidirectional Encoder Representations from Transformers) with 340 million parameters and GPT-3 with 175 billion parameters.\\n\\nModern NLP applications include machine translation, sentiment analysis, named entity recognition, question answering, and text summarization. Google Translate now handles over 100 languages, processing more than 100 billion words daily. ChatGPT, launched in November 2022, reached 100 million users in just 2 months, becoming the fastest-growing consumer application in history.\\nComputer Vision: Enabling Machines to See\\n\\nComputer vision empowers machines to interpret and understand visual information from the world. Convolutional Neural Networks (CNNs), inspired by the biological visual cortex, have become the standard architecture for image-related tasks. A typical CNN contains convolutional layers (feature extraction), pooling layers (dimensionality reduction), and fully connected layers (classification).\\n\\nObject detection systems like YOLO (You Only Look Once) can identify and locate multiple objects in images at speeds exceeding 30 frames per second, enabling real-time applications. Semantic segmentation models assign class labels to every pixel, achieving accuracy rates above 90% on benchmark datasets like PASCAL VOC and COCO.\\n\\nFacial recognition technology has achieved accuracy rates exceeding 99.7% on standard benchmarks, surpassing human performance in controlled conditions. However, concerns about bias and privacy have led to regulatory restrictions in several jurisdictions, with cities like San Francisco and Boston banning government use of facial recognition technology.\\nGenerative AI and Large Language Models\\n\\nGenerative AI represents a paradigm shift, creating new content rather than merely analyzing existing data. Generative Adversarial Networks (GANs), introduced by Ian Goodfellow in 2014, pit two neural networks—a generator and discriminator—against each other in a competitive game. GANs have produced photorealistic images, synthesized voices, and even generated entire video sequences.\\n\\nLarge Language Models (LLMs) like GPT-4, Claude, and PaLM demonstrate remarkable capabilities across diverse tasks. These models undergo two-stage training: pre-training on massive text corpora (often 1+ trillion tokens) to learn language patterns, followed by fine-tuning on specific tasks or alignment with human preferences through Reinforcement Learning from Human Feedback (RLHF).\\n\\nRetrieval-Augmented Generation (RAG) addresses the knowledge limitation of LLMs by combining them with external knowledge bases. RAG systems retrieve relevant documents from vector databases and inject this context into prompts, reducing hallucinations and enabling access to up-to-date or proprietary information. A typical RAG pipeline includes document ingestion, embedding generation, vector storage (using databases like Pinecone, Weaviate, or FAISS), retrieval, and generation—achieving accuracy improvements of 20-30% over standalone LLMs on knowledge-intensive tasks.\\nAI in Healthcare: Transforming Medicine\\n\\nHealthcare has emerged as one of AI\\'s most promising application domains. Diagnostic systems now detect diseases with accuracy rivaling or exceeding human experts. Google\\'s DeepMind developed an AI system that identifies over 50 eye diseases from retinal scans with 94% accuracy, matching the performance of world-leading ophthalmologists.\\n\\nIn radiology, AI models analyze X-rays, CT scans, and MRIs to detect abnormalities like tumors, fractures, and lesions. A study published in Nature in 2020 showed that an AI system detected breast cancer in mammograms with 89% accuracy, reducing false positives by 5.7% and false negatives by 9.4% compared to human radiologists.\\n\\nDrug discovery has accelerated dramatically through AI. Traditional drug development takes 10-15 years and costs approximately $2.6 billion. AI platforms from companies like Atomwise, Insilico Medicine, and BenevolentAI reduce this timeline to 2-3 years by predicting molecular properties, optimizing chemical structures, and identifying promising drug candidates from millions of compounds.\\nEthical Considerations and Challenges\\n\\nDespite remarkable progress, AI faces significant ethical challenges. Algorithmic bias perpetuates and amplifies societal prejudices when models train on biased historical data. Amazon discontinued an AI recruiting tool in 2018 after discovering it discriminated against women, having learned from historical hiring patterns that favored men.\\n\\nPrivacy concerns intensify as AI systems process sensitive personal data. The European Union\\'s General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) impose strict requirements on data collection, storage, and usage. Techniques like differential privacy and federated learning enable AI training while preserving individual privacy.\\n\\nThe \"black box\" problem hampers AI adoption in critical domains like healthcare and criminal justice. Explainable AI (XAI) methods like LIME, SHAP, and attention visualization help humans understand model decisions, but fully transparent deep learning remains an open research challenge.\\nThe Future Landscape\\n\\nThe AI market is projected to reach $1.8 trillion by 2030, growing at a compound annual growth rate of 37.3%. Emerging trends include multimodal AI (processing text, images, audio, and video simultaneously), neuromorphic computing (brain-inspired hardware), and quantum machine learning.\\n\\nArtificial General Intelligence (AGI)—systems with human-level intelligence across all domains—remains speculative, with expert predictions ranging from 2030 to never. However, narrow AI continues advancing rapidly, with transformer models doubling in size every 3.4 months since 2018, following a trajectory even faster than Moore\\'s Law.')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0bce3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54c9183e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '../data/AI.txt'}, page_content='The Evolution and Impact of Artificial Intelligence: From Theory to Practice\\n\\nArtificial Intelligence (AI) has transformed from a theoretical concept in the 1950s into one of the most influential technologies of the 21st century. The field encompasses multiple disciplines including machine learning, natural language processing, computer vision, and robotics, with applications spanning healthcare, finance, education, and beyond.\\nHistorical Foundations and Key Milestones')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "461ee0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding the chunks\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17323a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vector store\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0657e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Large Language Models (LLMs) like GPT-4, Claude, and PaLM demonstrate remarkable capabilities across diverse tasks. These models undergo two-stage training: pre-training on massive text corpora (often 1+ trillion tokens) to learn language patterns, followed by fine-tuning on specific tasks or alignment with human preferences through Reinforcement Learning from Human Feedback (RLHF).\n",
      "**************************************************\n",
      "Document 2:\n",
      "The transformer architecture, introduced in the seminal 2017 paper \"Attention Is All You Need\" by Vaswani et al., revolutionized NLP. Transformers use self-attention mechanisms to process entire sequences simultaneously, enabling parallel computation and capturing long-range dependencies more effectively than recurrent neural networks (RNNs). This architecture spawned groundbreaking models like BERT (Bidirectional Encoder Representations from Transformers) with 340 million parameters and GPT-3 with 175 billion parameters.\n",
      "**************************************************\n",
      "Document 3:\n",
      "Modern NLP applications include machine translation, sentiment analysis, named entity recognition, question answering, and text summarization. Google Translate now handles over 100 languages, processing more than 100 billion words daily. ChatGPT, launched in November 2022, reached 100 million users in just 2 months, becoming the fastest-growing consumer application in history.\n",
      "Computer Vision: Enabling Machines to See\n",
      "**************************************************\n",
      "Document 4:\n",
      "Facial recognition technology has achieved accuracy rates exceeding 99.7% on standard benchmarks, surpassing human performance in controlled conditions. However, concerns about bias and privacy have led to regulatory restrictions in several jurisdictions, with cities like San Francisco and Boston banning government use of facial recognition technology.\n",
      "Generative AI and Large Language Models\n",
      "\n",
      "Generative AI represents a paradigm shift, creating new content rather than merely analyzing existing data. Generative Adversarial Networks (GANs), introduced by Ian Goodfellow in 2014, pit two neural networks—a generator and discriminator—against each other in a competitive game. GANs have produced photorealistic images, synthesized voices, and even generated entire video sequences.\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# fetching the most relevant documents to query from vector store\n",
    "query = \"How many parameters does GPT-3 have?\"\n",
    "docs = vectorstore.similarity_search(query, k=4)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a75acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the chat model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "972d2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the prompt template\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use ten sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "316ef3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse ten sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20c352d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining output parser\n",
    "str_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5bbc9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining chain\n",
    "chain = {'context': retriever, 'question': RunnablePassthrough()}| prompt| llm | str_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09777b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\masan\\OneDrive\\Desktop\\Advanced End to End RAG Application\\.venv\\Lib\\site-packages\\pydantic\\v1\\main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
      "            id = uuid7()\n",
      "Future versions will require UUID v7.\n",
      "  input_data = validator(cls_, input_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 has 175 billion parameters. This large number of parameters contributes to its ability to understand and generate human-like text across various tasks. The model's architecture is based on the transformer framework, which allows it to process sequences of text effectively. This capability is a significant advancement over previous models, enabling better performance in natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "# getting inference\n",
    "final_answer = chain.invoke(\"How many parameters does GPT-3 have?\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
